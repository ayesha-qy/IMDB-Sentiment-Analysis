{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZmt8MlULNF9",
    "outputId": "db269ddd-82bd-4a33-bdf1-430f5a38dabc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /Users/ayeshaqureshi/anaconda3/lib/python3.11/site-packages (2022.7.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FHMYkyZFLWn5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfEs6iXdMPjM",
    "outputId": "ad1457dc-9d18-4547-d212-595573ca89db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7339, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"/Users/ayeshaqureshi/Downloads/Data.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "aDSiH9WlMvhY"
   },
   "source": [
    "How many records have a date that is expressed without using alphabets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEKWWgzJMxcn",
    "outputId": "aafb2cb5-db0b-46ee-bfc4-49bbd7951c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records:  16 \n",
      "Value Matched:  {'2015-19'}\n"
     ]
    }
   ],
   "source": [
    "def date_no_alphabet(data):\n",
    "    text =[' '.join(item) for item in data.values.tolist()]\n",
    "    num_matches = 0\n",
    "    dates = []\n",
    "    for string in text:\n",
    "        string = string.lower()\n",
    "        match = re.search(r'\\b((\\d{1,2}[-/\\\\])?\\d{1,2}[-/\\\\]\\d{4}|\\d{4}[-/\\\\]\\d{1,2}([-/\\\\]\\d{1,2})?)\\b',string)\n",
    "        if match:\n",
    "            num_matches+=1\n",
    "            dates.append(match.group())\n",
    "    return num_matches,set(dates)\n",
    "\n",
    "number_dates,match = date_no_alphabet(data)\n",
    "print('Number of Records: ',number_dates,'\\nValue Matched: ',match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX4X13UzM3v2"
   },
   "source": [
    "Considering dates of the form:\n",
    "\n",
    "- dd/mm/yyyy\n",
    "- dd/mm/yy\n",
    "- mm/dd/yyyy\n",
    "- mm/dd/yy\n",
    "- mm/yy\n",
    "- mm/yyyy\n",
    "- yyyy/mm/dd\n",
    "- yyyy/dd/mm\n",
    "- yy/mm/dd\n",
    "- yy/dd/mm\n",
    "- yyyy/mm\n",
    "- yy/mm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sU3UWwe8M6BL"
   },
   "source": [
    "How many records have a word starting with the letter “w”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qC_3wNC8M4pm",
    "outputId": "42d25b86-cb1d-4105-ef32-d7493c0c2f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words starting with w:  4958\n"
     ]
    }
   ],
   "source": [
    "def w_word_count(data):\n",
    "    text =[' '.join(item) for item in data.values.tolist()]\n",
    "    count=0\n",
    "    for string in text:\n",
    "        string = string.lower()\n",
    "        match = re.findall(r'w[a-z0-9]+',string)\n",
    "        if match:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "w_words = w_word_count(data)\n",
    "print('Number of words starting with w: ',w_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pqYkMdQgM9RY"
   },
   "outputs": [],
   "source": [
    "data_list = data['Data'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "yOn6RNUFNAt5"
   },
   "source": [
    "How many records make a word that starts with an alphabet and is not a URL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBr68IybM_Eg",
    "outputId": "917770dd-1493-4e61-d828-6f0f12601300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with words start with an alphabet and are not URLs: 115835\n"
     ]
    }
   ],
   "source": [
    "def s_with_alphabet(data_list):\n",
    "    count = 0\n",
    "    url_pattern = r'(https?:\\/\\/|www\\.)?[a-zA-Z0-9]+\\.[^\\s]{2,}'\n",
    "    word_pattern = r'[a-zA-Z][a-zA-Z0-9]*'\n",
    "\n",
    "    for i in data_list:\n",
    "        excl_url = re.sub(url_pattern,'', i)\n",
    "\n",
    "        match =  re.findall(word_pattern,excl_url)\n",
    "        # print(match)\n",
    "        count+=len(match)\n",
    "    return count\n",
    "\n",
    "print('Number of records with words start with an alphabet and are not URLs:', s_with_alphabet(data_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "nvTdCvM9NHLb"
   },
   "source": [
    "How many tweets contain one of these emojis :), :D, ;), :P?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTxKRpHkNFRN",
    "outputId": "a6bc409e-99a7-4542-fc03-5fe800a5ddd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets contain these emojis :), :D, ;), :P: 18\n"
     ]
    }
   ],
   "source": [
    "def emoji(data_list):\n",
    "    reg = r'(:\\)|:D|;\\)|:P)'\n",
    "    count=0\n",
    "\n",
    "    for i in data_list:\n",
    "        if re.findall(reg, i):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "print('Number of tweets contain these emojis :), :D, ;), :P:', emoji(data_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ioB2PpHwNLsP"
   },
   "source": [
    "Count records containing a decimal number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sNum77DNKMN",
    "outputId": "d3857de0-d1a1-42ba-e1d6-2b3a3db930fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records containing a decimal number:  25\n"
     ]
    }
   ],
   "source": [
    "def decimal_number_count(data):\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each string in the list\n",
    "    for string in data_list:\n",
    "        # searches for record starting with decimal number\n",
    "        if re.search(r'\\b\\d+\\.\\d+\\b', string):\n",
    "            count += 1  # Increment count by 1 if there is a match\n",
    "\n",
    "    return count\n",
    "\n",
    "decimal_numbers = decimal_number_count(data)\n",
    "print('Number of records containing a decimal number: ', decimal_numbers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HYChqi46NnHW"
   },
   "source": [
    "No. of records with IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJPipTDxNlbR",
    "outputId": "d05a3b28-1666-4136-f1b0-020de74b7c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records containingIP address:  0\n"
     ]
    }
   ],
   "source": [
    "def IP_address_count(data):\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each string in the list\n",
    "    for string in data_list:\n",
    "        # searches for record with IP address\n",
    "        if re.search(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', string):\n",
    "            count += 1  # Increment count by 1 if there is a match\n",
    "\n",
    "    return count\n",
    "\n",
    "IP_address = IP_address_count(data)\n",
    "print('Number of records containingIP address: ', IP_address)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "nni7sUBYNxc6"
   },
   "source": [
    "How many records have a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rL-oyBD1Ntkt",
    "outputId": "eadf2908-c13b-441a-c9d3-76963d9957d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newline records 1211\n"
     ]
    }
   ],
   "source": [
    "def newline_records(data):\n",
    "    count = 0\n",
    "    for a in data_list:\n",
    "        if '\\n' in a:\n",
    "            count += 1\n",
    "    return count\n",
    "print('Number of newline records',newline_records(data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bQ7cll3rOER0"
   },
   "source": [
    "What is the total number of hastags across all these tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aillrlZjOCzI",
    "outputId": "104c530a-b265-47da-f2d1-e66bbd43e1af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hashtags record 2924\n"
     ]
    }
   ],
   "source": [
    "def number_of_hashtags(data):\n",
    "    count = 0\n",
    "    hashtag_type = r'#\\w+'\n",
    "    for a in data_list:\n",
    "        count += len(re.findall(hashtag_type, a)) #re.finall() finds all matches in each tweet\n",
    "    return count\n",
    "print('Number of hashtags record',number_of_hashtags(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-fBaWhWOSPQ"
   },
   "source": [
    "What is the code to substitute all non-alphanumeric characters with a new line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9n7Ykt9rGrM4",
    "outputId": "daa0e10a-cee3-4c4a-cc4d-f6884c80984e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Watch\n",
      "or\n",
      "listen\n",
      "live\n",
      "weekdays\n",
      "at\n",
      "8\n",
      "30am\n",
      "MT\n",
      "at\n",
      "ryanjespersen\n",
      "com\n",
      "Subscribe\n",
      "via\n",
      "YouTube\n",
      "or\n",
      "your\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "def sub_data():\n",
    "\n",
    "    with open('/content/NLP Ungraded.csv', 'r', encoding='utf-8') as content:\n",
    "        data = content.read()\n",
    "\n",
    "    # To substitute all the all non - numeric characters using (a-zA-Z0-9) with new line\n",
    "    data1 = re.sub(r'[^a-zA-Z0-9]+', '\\n', data)\n",
    "\n",
    "    data1 = '\\n'.join(line.strip() for line in data1.split('\\n') if line.strip())\n",
    "\n",
    "    # First 100 characters\n",
    "    print(data1[:100])\n",
    "\n",
    "sub_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds6fG138OdIM"
   },
   "source": [
    "What is the total number of URLs across all tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqpS45bxOjp6",
    "outputId": "a1619f4b-b2fc-407a-9740-e7a098593751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs found: 4\n"
     ]
    }
   ],
   "source": [
    "def count_urls(data):\n",
    "    url_exp = r'https?://[^\\s]+' #expression to match the URLs\n",
    "    count = 0\n",
    "    for tweets in data: #Iterate over each row of the data\n",
    "      count += len(re.findall(url_exp, tweets)) #Find the respective URLs in tweets\n",
    "    return count\n",
    "print('Number of URLs found:', count_urls(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8l1_B6c-HHqs"
   },
   "source": [
    "Perform stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ofIC-lKZQYgg"
   },
   "outputs": [],
   "source": [
    "def clean_text(token):\n",
    "    cleaned_tokens = [word for word in token if word.isalpha()]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeaDqqNcHhov",
    "outputId": "f4f59436-e6b9-4e1e-d244-07142754cbc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7339, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/content/NLP Ungraded.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "F6qm8F0lHRUW"
   },
   "source": [
    "Use porter stemmer to run stemming. Count the number of unique words/tokens before and after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tovjy2I2HNt1",
    "outputId": "6cc0846b-c006-4725-c4d2-056b3bc118c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbT88_Z1HX9F",
    "outputId": "5d656818-a374-4b51-d3cb-2729680d7f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens before stemming: 7119\n",
      "Unique tokens after stemming: 5820\n"
     ]
    }
   ],
   "source": [
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    initial_tokens = [] #to store all the tokens\n",
    "    global stemmed_tokens\n",
    "    stemmed_tokens = [] #to store stemmed tokens\n",
    "\n",
    "    for t in text:\n",
    "        tokens = nltk.tokenize.word_tokenize(t.lower()) #converting to lower case and tokenizing\n",
    "        tokens = clean_text(tokens)\n",
    "        initial_tokens.extend(tokens) #storing all the tokens\n",
    "        stemmed_tokens.extend(ps.stem(token) for token in tokens) #performing stemming and storing stemmed tokens\n",
    "\n",
    "    return len(set(initial_tokens)), len(set(stemmed_tokens)) #returning lengths of tokens\n",
    "\n",
    "unique_before_stemming, unique_after_stemming = stemming(df['Data']) #function call\n",
    "print(f\"Unique tokens before stemming: {unique_before_stemming}\")\n",
    "print(f\"Unique tokens after stemming: {unique_after_stemming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhMixYkXHqHP"
   },
   "source": [
    "Perform lemmatization using NLTK lemmatizer. Count the number of unique words/tokens before and after lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ipA1nxPHyt2",
    "outputId": "d0036c77-dbe8-48a0-81cb-43d5108708f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkKdyb7UHaxa",
    "outputId": "d6807fb9-0999-4574-eee7-5cd8bed1301d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens before lemmatizing: 7119\n",
      "Unique tokens after lemmatizing: 6587\n"
     ]
    }
   ],
   "source": [
    "def lemmatizing(text):\n",
    "    lm = WordNetLemmatizer()\n",
    "    initial_tokens = [] #to store all the tokens\n",
    "    global lemmatized_tokens\n",
    "    lemmatized_tokens = [] #to store lemmatized tokens\n",
    "\n",
    "    for t in text:\n",
    "        tokens = nltk.tokenize.word_tokenize(t.lower()) #converting to lower case and tokenizing\n",
    "        tokens = clean_text(tokens)\n",
    "\n",
    "        initial_tokens.extend(tokens) #storing all the tokens\n",
    "        lemmatized_tokens.extend(lm.lemmatize(token) for token in tokens) #performing lemmatization and storing lemmatized tokens\n",
    "    return len(set(initial_tokens)), len(set(lemmatized_tokens)) #returning lengths of tokens\n",
    "\n",
    "unique_before_lemmatizing, unique_after_lemmatizing = lemmatizing(df['Data']) #function call\n",
    "print(f\"Unique tokens before lemmatizing: {unique_before_lemmatizing}\") #printing results\n",
    "print(f\"Unique tokens after lemmatizing: {unique_after_lemmatizing}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "r8aUdxXgH3My"
   },
   "source": [
    "Comparing change in word frequencies with both Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS1d7TpZHvvP",
    "outputId": "006b3b11-befd-4dc2-c7ec-6d842dfd1f8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 3514),\n",
       " ('the', 2859),\n",
       " ('of', 2223),\n",
       " ('to', 2179),\n",
       " ('a', 1768),\n",
       " ('in', 1541),\n",
       " ('for', 1507),\n",
       " ('i', 1189),\n",
       " ('alberta', 1061),\n",
       " ('is', 923)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Top 10 words after lemmatization\n",
    "wordcount = dict(Counter(lemmatized_tokens))\n",
    "sorted_values1 = list(sorted(wordcount.items(),key = lambda item: item[1],reverse=True))\n",
    "sorted_values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eQyd_ChH8U2",
    "outputId": "d6447916-8a92-4a1f-fc43-b2f52bd2b109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 3514),\n",
       " ('the', 2859),\n",
       " ('of', 2223),\n",
       " ('to', 2179),\n",
       " ('a', 1551),\n",
       " ('in', 1541),\n",
       " ('for', 1507),\n",
       " ('i', 1189),\n",
       " ('alberta', 1061),\n",
       " ('is', 923)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOp 10 words after Stemming\n",
    "wordcount = dict(Counter(stemmed_tokens))\n",
    "sorted_values2 = list(sorted(wordcount.items(),key = lambda item: item[1],reverse=True))\n",
    "sorted_values2[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "VWYgx9SzIBwX"
   },
   "source": [
    "After Normalization and stopword removal, comparing word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55osbgLiH_bk",
    "outputId": "9a378098-59a8-4215-ccc1-46a5048fd6f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qPfeymE_IL0X"
   },
   "outputs": [],
   "source": [
    "def clean_text(token):\n",
    "    cleaned_tokens = [word for word in token if word.isalpha()]          # keeping only aphabets\n",
    "    cleaned_tokens = [word for word in cleaned_tokens if word not in stop_words]        # removing stopwords\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNFeJONBIOf1",
    "outputId": "87f19321-5362-47c1-db46-aa5c1e634dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens before stemming: 6992\n",
      "Unique tokens after stemming: 5707\n"
     ]
    }
   ],
   "source": [
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    initial_tokens = [] #to store all the tokens\n",
    "    global stemmed_tokens\n",
    "    stemmed_tokens = [] #to store stemmed tokens\n",
    "\n",
    "    for t in text:\n",
    "        tokens = nltk.tokenize.word_tokenize(t.lower()) #converting to lower case and tokenizing\n",
    "        tokens = clean_text(tokens)\n",
    "        initial_tokens.extend(tokens) #storing all the tokens\n",
    "        stemmed_tokens.extend(ps.stem(token) for token in tokens) #performing stemming and storing stemmed tokens\n",
    "\n",
    "    return len(set(initial_tokens)), len(set(stemmed_tokens)) #returning lengths of tokens\n",
    "\n",
    "unique_before_stemming, unique_after_stemming = stemming(df['Data']) #function call\n",
    "print(f\"Unique tokens before stemming: {unique_before_stemming}\")\n",
    "print(f\"Unique tokens after stemming: {unique_after_stemming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7VmEMh9QIQQi",
    "outputId": "bf4388d4-89e5-4998-b829-12e072b4cc01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens before lemmatizing: 6992\n",
      "Unique tokens after lemmatizing: 6465\n"
     ]
    }
   ],
   "source": [
    "def lemmatizing(text):\n",
    "    lm = WordNetLemmatizer()\n",
    "    initial_tokens = [] #to store all the tokens\n",
    "    global lemmatized_tokens\n",
    "    lemmatized_tokens = [] #to store lemmatized tokens\n",
    "\n",
    "    for t in text:\n",
    "        tokens = nltk.tokenize.word_tokenize(t.lower()) #converting to lower case and tokenizing\n",
    "        tokens = clean_text(tokens)\n",
    "\n",
    "        initial_tokens.extend(tokens) #storing all the tokens\n",
    "        lemmatized_tokens.extend(lm.lemmatize(token) for token in tokens) #performing lemmatization and storing lemmatized tokens\n",
    "    return len(set(initial_tokens)), len(set(lemmatized_tokens)) #returning lengths of tokens\n",
    "\n",
    "unique_before_lemmatizing, unique_after_lemmatizing = lemmatizing(df['Data']) #function call\n",
    "print(f\"Unique tokens before lemmatizing: {unique_before_lemmatizing}\") #printing results\n",
    "print(f\"Unique tokens after lemmatizing: {unique_after_lemmatizing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evzZnr0GIT3w",
    "outputId": "a6450873-fe9b-4da4-f3a2-3118f56e2448"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alberta', 1061),\n",
       " ('news', 604),\n",
       " ('canada', 559),\n",
       " ('outdoor', 549),\n",
       " ('community', 542),\n",
       " ('adventure', 516),\n",
       " ('calgary', 390),\n",
       " ('writer', 385),\n",
       " ('park', 380),\n",
       " ('love', 361)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words after lemmatization\n",
    "wordcount = dict(Counter(lemmatized_tokens))\n",
    "sorted_values1 = list(sorted(wordcount.items(),key = lambda item: item[1],reverse=True))\n",
    "sorted_values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgNpjKYPIXKf",
    "outputId": "9cb09075-bcf4-4fcf-8e45-cf1d25788d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alberta', 1061),\n",
       " ('outdoor', 682),\n",
       " ('news', 604),\n",
       " ('commun', 595),\n",
       " ('canada', 559),\n",
       " ('adventur', 530),\n",
       " ('travel', 459),\n",
       " ('follow', 446),\n",
       " ('love', 410),\n",
       " ('calgari', 390)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOp 10 words after Stemming\n",
    "wordcount = dict(Counter(stemmed_tokens))\n",
    "sorted_values2 = list(sorted(wordcount.items(),key = lambda item: item[1],reverse=True))\n",
    "sorted_values2[:10]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
